{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60259ae0",
   "metadata": {},
   "source": [
    "du 01/07/2023   \n",
    "objectif : évaluer l'influence du modèle de langue et du bruit sur l'entity liking.    \n",
    "contexte : reprise de \"testELSpacy_test-modèles_langues.ipynb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f51eb",
   "metadata": {},
   "source": [
    "# Evaluation comparative  du système REFINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4729014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher (txt):\n",
    "    from refined.inference.processor import Refined\n",
    "\n",
    "\n",
    "    refined = Refined.from_pretrained(model_name='wikipedia_model_with_numbers',\n",
    "                                  entity_set=\"wikipedia\")\n",
    "\n",
    "    spans = refined.process_text(txt)\n",
    "\n",
    "    print(spans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc1c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "caroline = \"Je m'appelle Caroline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003e32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonomaz/anaconda3/envs/refined38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/antonomaz/anaconda3/envs/refined38/lib/python3.8/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Je m'appelle Caroline\", Entity not linked to a knowledge base, 'WORK_OF_ART']]\n"
     ]
    }
   ],
   "source": [
    "afficher (caroline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61fca6",
   "metadata": {},
   "source": [
    "# Discord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7272f7ef",
   "metadata": {},
   "source": [
    "## Page Wikipédia Discord en anglais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e396db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "discord = \"Discord is an American VoIP and instant messaging social platform. Discord runs on Windows, macOS, Android, iOS, iPadOS, Linux, and in web browsers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde8f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Discord', Entity(wikidata_entity_id=Q22907849, wikipedia_entity_title=Discord (software)), 'ORG'], ['American', Entity(wikidata_entity_id=Q30, wikipedia_entity_title=United States), 'GPE'], ['VoIP', Entity(wikidata_entity_id=Q81945, wikipedia_entity_title=Voice over IP), None], ['instant messaging', Entity(wikidata_entity_id=Q58199, wikipedia_entity_title=Instant messaging), None], ['Discord', Entity(wikidata_entity_id=Q22907849, wikipedia_entity_title=Discord (software)), 'ORG'], ['Windows', Entity(wikidata_entity_id=Q1406, wikipedia_entity_title=Microsoft Windows), None], ['macOS', Entity(wikidata_entity_id=Q14116, wikipedia_entity_title=MacOS), None], ['Android', Entity(wikidata_entity_id=Q94, wikipedia_entity_title=Android (operating system)), None], ['iOS', Entity(wikidata_entity_id=Q48493, wikipedia_entity_title=IOS), None], ['iPadOS', Entity(wikidata_entity_id=Q64350339, wikipedia_entity_title=IPadOS), None], ['Linux', Entity(wikidata_entity_id=Q388, wikipedia_entity_title=Linux), None]]\n"
     ]
    }
   ],
   "source": [
    "afficher (discord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c99367",
   "metadata": {},
   "source": [
    "## page wikipédia discord en fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b16d55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "discord_fr = \"Discord est un logiciel propriétaire gratuit de VoIP et de messagerie instantanée. Il fonctionne sur les systèmes d’exploitation Windows, macOS, Linux, Android, iOS ainsi que sur les navigateurs web.  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c016002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Windows', Entity(wikidata_entity_id=Q1406, wikipedia_entity_title=Microsoft Windows), None], ['macOS', Entity(wikidata_entity_id=Q14116, wikipedia_entity_title=MacOS), None], ['Linux', Entity(wikidata_entity_id=Q388, wikipedia_entity_title=Linux), None], ['Android', Entity(wikidata_entity_id=Q94, wikipedia_entity_title=Android (operating system)), None], ['iOS', Entity(wikidata_entity_id=Q48493, wikipedia_entity_title=IOS), None]]\n"
     ]
    }
   ],
   "source": [
    "afficher(discord_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f626f7",
   "metadata": {},
   "source": [
    "## Nabilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5e4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "nabilla =\"En novembre 2013, elle est la vedette de sa propre émission de téléréalité, Allô Nabilla, ce qui est une première en France, le concept étant repris des États-Unis où l’on suit la vie des stars, notamment comme avec Kesha, Paris Hilton ou les Kardashian. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c980d81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['En', None, 'DATE'], ['novembre 2013', None, 'DATE'], ['téléréalité', Entity not linked to a knowledge base, 'ORG'], ['Allô Nabilla', Entity not linked to a knowledge base, 'PERSON'], ['France', Entity(wikidata_entity_id=Q142, wikipedia_entity_title=France), 'ORG'], ['États-Unis', Entity(wikidata_entity_id=Q30, wikipedia_entity_title=United States), 'ORG'], ['Kesha', Entity(wikidata_entity_id=Q33605, wikipedia_entity_title=Kesha), 'PERSON'], ['Paris Hilton', Entity(wikidata_entity_id=Q47899, wikipedia_entity_title=Paris Hilton), 'PERSON'], ['Kardashian', Entity(wikidata_entity_id=Q186304, wikipedia_entity_title=Kim Kardashian), 'PERSON']]\n"
     ]
    }
   ],
   "source": [
    "afficher(nabilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0475ac2",
   "metadata": {},
   "source": [
    "## Pirates des Caraibes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e7473c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pirates = \"I watched the Pirates of the Caribbean last silvester\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a1cde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Pirates of the Caribbean', Entity not linked to a knowledge base, None]]\n"
     ]
    }
   ],
   "source": [
    "afficher(pirates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7e681",
   "metadata": {},
   "source": [
    "# Napoléon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46f7ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "napoleon_fr= \"Napoléon Bonaparte a envahi la Russie en 1812\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c9574b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Napoléon Bonaparte', Entity(wikidata_entity_id=Q517, wikipedia_entity_title=Napoleon), None], ['Russie', Entity(wikidata_entity_id=Q159, wikipedia_entity_title=Russia), 'ORG'], ['1812', Entity(parsed_string=[timepoint: [\"1812\"]]), 'DATE']]\n"
     ]
    }
   ],
   "source": [
    "afficher (napoleon_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2de68c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "napoleon_eng= \"Napoléon Bonaparte envaded Russia in 1812\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "051039fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Napoléon Bonaparte', Entity(wikidata_entity_id=Q517, wikipedia_entity_title=Napoleon), 'PERSON'], ['Russia', Entity(wikidata_entity_id=Q159, wikipedia_entity_title=Russia), 'ORG'], ['1812', Entity(parsed_string=[timepoint: [\"1812\"]]), 'DATE']]\n"
     ]
    }
   ],
   "source": [
    "afficher(napoleon_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b99da",
   "metadata": {},
   "source": [
    "# BRONTE, Wuthering heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3e33325",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"A Pious Discourse delivered by the Reverend Jabes Branderham, in the Chapel of Gimmerden Sough\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "859a912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken = \"A Pious Discourse delisrered by the Reverend Jabes Branderham, in the Chapel of Gimmer-den Songh.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "209cdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseractBin=\"A Pious Discourse delivered by the Reverend Jabes Branderham, in the Chapel of Gimmer-den Sough.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f06ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseractPNG =\"A Pious Discourse delivered by the Reverend Jabes Branderham, in the Chapel of Gimmer-den Sough\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a84559d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A Pious Discourse', Entity not linked to a knowledge base, None], ['Jabes Branderham', Entity not linked to a knowledge base, 'PERSON'], ['Chapel of Gimmer-den Songh', Entity not linked to a knowledge base, 'FAC']]\n"
     ]
    }
   ],
   "source": [
    "afficher(kraken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bbbb2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A Pious Discourse', Entity not linked to a knowledge base, None], ['Jabes Branderham', Entity not linked to a knowledge base, 'PERSON'], ['Chapel of Gimmer-den Songh', Entity not linked to a knowledge base, 'FAC']]\n"
     ]
    }
   ],
   "source": [
    "afficher(kraken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6df6fe",
   "metadata": {},
   "source": [
    "# AINSWORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "010e25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"he declares he lived in Queen Bess’s time, recollects King Charles bein’ beheaded perfectly vell, and remembers the Great Fire o’ London” \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a08e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken = \"e declares ho lired in (Queen Bess's time, recollects Iing Charles bein' beheaded perfectly vell, and remembers,the Great Fire o' London\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "800469ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseractBIN = \"he declares he lived in Queen Bess’s time, recollects King Charles bein’ beheaded perfectly vell, and remembers the Great Fire o’ London, as if it only occurred yester-day.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26751df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseractPNG = \"he declares he lived in Queen Bess’s time, recollects King Charles bein’ beheaded perfectly vell, and remembers the Great Fire o’ London\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4404b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Bess', Entity(wikidata_entity_id=Q1331334, wikipedia_entity_title=Elizabeth Raleigh), 'PERSON'], ['Charles', Entity(wikidata_entity_id=Q81506, wikipedia_entity_title=Charles I of England), 'PERSON'], ['the Great Fire o’', Entity not linked to a knowledge base, None], ['London', Entity(wikidata_entity_id=Q84, wikipedia_entity_title=London), 'GPE']]\n"
     ]
    }
   ],
   "source": [
    "afficher(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1698c76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e', Entity not linked to a knowledge base, None], ['Bess', Entity not linked to a knowledge base, 'PERSON'], ['Iing Charles', Entity not linked to a knowledge base, 'PERSON'], [\"the Great Fire o' London\", Entity not linked to a knowledge base, None]]\n"
     ]
    }
   ],
   "source": [
    "afficher(kraken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3f02a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllr}\n",
      "\\toprule\n",
      "span & label  & description & ID & score \\\\\n",
      "\\midrule\n",
      "Queen Bess & PERSON & ['Queen of England and Ireland from 1558 to 1603'] & Q7207 & 0.510875 \\\\\n",
      "London & LOC & ['capital and largest city of the United Kingdom'] & Q84 & 0.230175 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latexiser_eng(kraken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4161b638",
   "metadata": {},
   "source": [
    "# Maupassant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20a6fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref = \"Ils gagnèrent Livourne, visitèrent Florence, Gênes, toute la Corniche.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2d55732",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"Le baron Simon-Jacques Le Perthuis des Vauds était un gentilhomme de l'autre siècle, maniaque et bon. Disciple enthousiaste de J. -J. Rousseau, il avait des tendresses d'amant pour la nature, les champs, les bois, les bêtes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09852277",
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken = \" Le baron Simon-lacques Le Perthuis des Vauds etait un gentilhomme de l'autre siecle, maniaque etbon. Disciple enthousiaste de J.-J. Rousseau, il avaitdes tendresses d'amant pour la nature, les champs, les bois, les betes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "216d1bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Simon-Jacques Le Perthuis', Entity not linked to a knowledge base, 'PERSON'], ['Vauds', Entity not linked to a knowledge base, 'ORG'], ['l', None, 'DATE'], ['autre siècle', None, 'DATE'], ['J. -J. Rousseau', Entity not linked to a knowledge base, 'PERSON']]\n"
     ]
    }
   ],
   "source": [
    "afficher(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4f1a5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Le baron Simon-lacques Le Perthuis des Vauds', Entity not linked to a knowledge base, 'PERSON'], ['J.-J. Rousseau', Entity(wikidata_entity_id=Q6527, wikipedia_entity_title=Jean-Jacques Rousseau), 'PERSON']]\n"
     ]
    }
   ],
   "source": [
    "afficher(kraken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9c75f7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllr}\n",
      "\\toprule\n",
      "span & label  & description & ID & score \\\\\n",
      "\\midrule\n",
      "Rousseau & PERSON & ['Genevan philosopher, writer and composer (1712–1778)'] & Q6527 & 0.090920 \\\\\n",
      "les champs & LOC & ['commune in Cher, France'] & Q965691 & 0.577959 \\\\\n",
      "les & LOC & ['sovereign state in southern Africa'] & Q1013 & 0.703957 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latexiser_fr(kraken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9590dc",
   "metadata": {},
   "source": [
    "# Noaille "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c042672",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "592cd10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken = \"ils allerent seuls l'un et l'autre, secre-tement, se cachant des curiosites de leurs amis, diner dans de petits cafes de Paris, oifloiait une odeur basse d'alcool et de tabac,et dans les restaurants du Bois de Boulogne,pleins de lumiere et de gens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c023299",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"ils allèrent seuls l'un et l'autre, secrètement, se cachant des curiosités de leurs amis, dîner dans de petits cafés de Paris, où flottait une odeur basse d'alcool et de tabac, et dans les restaurants du Bois de Boulogne, pleins de lumière et de gens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e67252df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['is', Entity not linked to a knowledge base, None], ['petits', Entity(wikidata_entity_id=Q7178382, wikipedia_entity_title=Petit baronets), 'ORG'], ['és', Entity not linked to a knowledge base, 'ORG'], ['Paris', Entity(wikidata_entity_id=Q90, wikipedia_entity_title=Paris), 'GPE'], [\"basse d'alcool\", Entity not linked to a knowledge base, None], ['tabac', Entity not linked to a knowledge base, 'ORG'], ['Bois de Boulogne', Entity(wikidata_entity_id=Q209626, wikipedia_entity_title=Bois de Boulogne), 'GPE'], ['pleins de lumière', Entity not linked to a knowledge base, 'FAC'], ['de gens', Entity not linked to a knowledge base, 'ORG']]\n"
     ]
    }
   ],
   "source": [
    "afficher(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "45fc4868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllr}\n",
      "\\toprule\n",
      "modèle & span & label  & description & ID & score \\\\\n",
      "\\midrule\n",
      "fr_core_news_sm & dans & ORG & ['organization'] & Q13570995 & 0.072841 \\\\\n",
      "fr_core_news_sm & Paris & LOC & ['capital and largest city of France'] & Q90 & 0.546902 \\\\\n",
      "fr_core_news_sm & dans & ORG & ['organization'] & Q13570995 & 0.092083 \\\\\n",
      "fr_core_news_sm & les & LOC & ['sovereign state in southern Africa'] & Q1013 & 0.499807 \\\\\n",
      "fr_core_news_sm & Bois de Boulogne & LOC & ['large public park the western edge of Paris, France'] & Q209626 & 0.841223 \\\\\n",
      "fr_core_news_md & dans & ORG & ['organization'] & Q13570995 & 0.072841 \\\\\n",
      "fr_core_news_md & Paris & LOC & ['capital and largest city of France'] & Q90 & 0.546902 \\\\\n",
      "fr_core_news_md & dans & ORG & ['organization'] & Q13570995 & 0.092083 \\\\\n",
      "fr_core_news_md & les & LOC & ['sovereign state in southern Africa'] & Q1013 & 0.499807 \\\\\n",
      "fr_core_news_md & Bois de Boulogne & LOC & ['large public park the western edge of Paris, France'] & Q209626 & 0.841223 \\\\\n",
      "fr_core_news_lg & dans & ORG & ['organization'] & Q13570995 & 0.072841 \\\\\n",
      "fr_core_news_lg & Paris & LOC & ['capital and largest city of France'] & Q90 & 0.546902 \\\\\n",
      "fr_core_news_lg & dans & ORG & ['organization'] & Q13570995 & 0.092083 \\\\\n",
      "fr_core_news_lg & les & LOC & ['sovereign state in southern Africa'] & Q1013 & 0.499807 \\\\\n",
      "fr_core_news_lg & Bois de Boulogne & LOC & ['large public park the western edge of Paris, France'] & Q209626 & 0.841223 \\\\\n",
      "en_core_web_sm & dans & ORG & ['organization'] & Q13570995 & 0.072841 \\\\\n",
      "en_core_web_sm & Paris & LOC & ['capital and largest city of France'] & Q90 & 0.546902 \\\\\n",
      "en_core_web_sm & dans & ORG & ['organization'] & Q13570995 & 0.092083 \\\\\n",
      "en_core_web_sm & les & LOC & ['sovereign state in southern Africa'] & Q1013 & 0.499807 \\\\\n",
      "en_core_web_sm & Bois de Boulogne & LOC & ['large public park the western edge of Paris, France'] & Q209626 & 0.841223 \\\\\n",
      "en_core_web_md & dans & ORG & ['organization'] & Q13570995 & 0.072841 \\\\\n",
      "en_core_web_md & Paris & LOC & ['capital and largest city of France'] & Q90 & 0.546902 \\\\\n",
      "en_core_web_md & dans & ORG & ['organization'] & Q13570995 & 0.092083 \\\\\n",
      "en_core_web_md & les & LOC & ['sovereign state in southern Africa'] & Q1013 & 0.499807 \\\\\n",
      "en_core_web_md & Bois de Boulogne & LOC & ['large public park the western edge of Paris, France'] & Q209626 & 0.841223 \\\\\n",
      "en_core_web_lg & dans & ORG & ['organization'] & Q13570995 & 0.072841 \\\\\n",
      "en_core_web_lg & Paris & LOC & ['capital and largest city of France'] & Q90 & 0.546902 \\\\\n",
      "en_core_web_lg & dans & ORG & ['organization'] & Q13570995 & 0.092083 \\\\\n",
      "en_core_web_lg & les & LOC & ['sovereign state in southern Africa'] & Q1013 & 0.499807 \\\\\n",
      "en_core_web_lg & Bois de Boulogne & LOC & ['large public park the western edge of Paris, France'] & Q209626 & 0.841223 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latexiser_mod(kraken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "10a21452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllr}\n",
      "\\toprule\n",
      "span & label  & description & ID & score \\\\\n",
      "\\midrule\n",
      "dans & ORG & ['organization'] & Q13570995 & 0.084679 \\\\\n",
      "Paris & LOC & ['capital and largest city of France'] & Q90 & 0.542768 \\\\\n",
      "dans & ORG & ['organization'] & Q13570995 & 0.087426 \\\\\n",
      "les & LOC & ['sovereign state in southern Africa'] & Q1013 & 0.498033 \\\\\n",
      "Bois de Boulogne & LOC & ['large public park the western edge of Paris, France'] & Q209626 & 0.812949 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latexiser_fr(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "14f0b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllr}\n",
      "\\toprule\n",
      "span & label  & description & ID & score \\\\\n",
      "\\midrule\n",
      "dans & ORG & ['organization'] & Q13570995 & 0.072841 \\\\\n",
      "Paris & LOC & ['capital and largest city of France'] & Q90 & 0.546902 \\\\\n",
      "dans & ORG & ['organization'] & Q13570995 & 0.092083 \\\\\n",
      "les & LOC & ['sovereign state in southern Africa'] & Q1013 & 0.499807 \\\\\n",
      "Bois de Boulogne & LOC & ['large public park the western edge of Paris, France'] & Q209626 & 0.841223 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latexiser_fr(kraken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed961e2b",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c60bc01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"Quand Philippe entra, madame de Fontenay, décidée à ne plus être gênée, se mit à parler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "879fa81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken = \"(Quand Philippe entra, madame de Fontenay, decidee a n6 plus etre genee, se mit a parler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8a019cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllr}\n",
      "\\toprule\n",
      "modèle & span & label  & description & ID & score \\\\\n",
      "\\midrule\n",
      "fr_core_news_sm & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "fr_core_news_md & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "fr_core_news_lg & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "en_core_web_sm & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "en_core_web_md & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "en_core_web_lg & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latexiser_mod(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916857ba",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "56c4b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken = \"Pierre Valenee criait, il s'echauffait sur la Revolution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7ac4fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"Pierre Valence criait, il s'échauffait sur la Révolution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "eb592569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllr}\n",
      "\\toprule\n",
      "span & label  & description & ID & score \\\\\n",
      "\\midrule\n",
      "Pierre Valenee & PER & NaN &  & NaN \\\\\n",
      "sur & LOC & ['country in South America'] & Q730 & 0.289179 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latexiser_fr(kraken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "29502e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllr}\n",
      "\\toprule\n",
      "span & label  & description & ID & score \\\\\n",
      "\\midrule\n",
      "Pierre Valence & PER & NaN &  & NaN \\\\\n",
      "sur & LOC & ['country in South America'] & Q730 & 0.357018 \\\\\n",
      "la Révolution & LOC & ['commune in Vendée, France'] & Q1421948 & 0.003900 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latexiser_fr(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5b025201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllr}\n",
      "\\toprule\n",
      "modèle & span & label  & description & ID & score \\\\\n",
      "\\midrule\n",
      "fr_core_news_sm & Pierre Valence & PER & NaN &  & NaN \\\\\n",
      "fr_core_news_sm & sur & LOC & ['country in South America'] & Q730 & 0.357018 \\\\\n",
      "fr_core_news_sm & la Révolution & LOC & ['commune in Vendée, France'] & Q1421948 & 0.003900 \\\\\n",
      "fr_core_news_md & Pierre Valence & PER & NaN &  & NaN \\\\\n",
      "fr_core_news_md & sur & LOC & ['country in South America'] & Q730 & 0.357018 \\\\\n",
      "fr_core_news_md & la Révolution & LOC & ['commune in Vendée, France'] & Q1421948 & 0.003900 \\\\\n",
      "fr_core_news_lg & Pierre Valence & PER & NaN &  & NaN \\\\\n",
      "fr_core_news_lg & sur & LOC & ['country in South America'] & Q730 & 0.357018 \\\\\n",
      "fr_core_news_lg & la Révolution & LOC & ['commune in Vendée, France'] & Q1421948 & 0.003900 \\\\\n",
      "en_core_web_sm & sur & LOC & ['country in South America'] & Q730 & 0.357018 \\\\\n",
      "en_core_web_sm & la Révolution & LOC & ['commune in Vendée, France'] & Q1421948 & 0.003900 \\\\\n",
      "en_core_web_md & sur & LOC & ['country in South America'] & Q730 & 0.357018 \\\\\n",
      "en_core_web_md & la Révolution & LOC & ['commune in Vendée, France'] & Q1421948 & 0.003900 \\\\\n",
      "en_core_web_lg & sur & LOC & ['country in South America'] & Q730 & 0.357018 \\\\\n",
      "en_core_web_lg & la Révolution & LOC & ['commune in Vendée, France'] & Q1421948 & 0.003900 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latexiser_mod(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cf70b",
   "metadata": {},
   "source": [
    "# AIMARD, les trappeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5bef2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"voilà une ville fondée à laquelle on donne un nom sonore comme Utique ou Syracuse, Rome ou Carthage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f80f5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken = \"voila une ville fondee a laquelle on donne un nom sonore comme Utique ou Syracuse, ome ou Carthage, par exemple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "3ee68c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____ ____     ____ ____ ____ ____     _  _ ____ _ _ _ ____     ____ _  _ \n",
      "|___ |__/     |    |  | |__/ |___     |\\ | |___ | | | [__      [__  |\\/| \n",
      "|    |  \\ ___ |___ |__| |  \\ |___ ___ | \\| |___ |_|_| ___] ___ ___] |  | \n",
      "                                                                         \n",
      "\n",
      "('Rome', 'Q220', 'LOC', ['capital and largest city of Italy'], 0.6630820438808648)\n",
      "____ ____     ____ ____ ____ ____     _  _ ____ _ _ _ ____     _  _ ___  \n",
      "|___ |__/     |    |  | |__/ |___     |\\ | |___ | | | [__      |\\/| |  \\ \n",
      "|    |  \\ ___ |___ |__| |  \\ |___ ___ | \\| |___ |_|_| ___] ___ |  | |__/ \n",
      "                                                                         \n",
      "\n",
      "('Rome', 'Q220', 'LOC', ['capital and largest city of Italy'], 0.6630820438808648)\n",
      "____ ____     ____ ____ ____ ____     _  _ ____ _ _ _ ____     _    ____ \n",
      "|___ |__/     |    |  | |__/ |___     |\\ | |___ | | | [__      |    | __ \n",
      "|    |  \\ ___ |___ |__| |  \\ |___ ___ | \\| |___ |_|_| ___] ___ |___ |__] \n",
      "                                                                         \n",
      "\n",
      "('Rome', 'Q220', 'LOC', ['capital and largest city of Italy'], 0.6630820438808648)\n",
      "____ _  _     ____ ____ ____ ____     _ _ _ ____ ___      ____ _  _ \n",
      "|___ |\\ |     |    |  | |__/ |___     | | | |___ |__]     [__  |\\/| \n",
      "|___ | \\| ___ |___ |__| |  \\ |___ ___ |_|_| |___ |__] ___ ___] |  | \n",
      "                                                                    \n",
      "\n",
      "('Rome', 'Q220', 'LOC', ['capital and largest city of Italy'], 0.6630820438808648)\n",
      "____ _  _     ____ ____ ____ ____     _ _ _ ____ ___      _  _ ___  \n",
      "|___ |\\ |     |    |  | |__/ |___     | | | |___ |__]     |\\/| |  \\ \n",
      "|___ | \\| ___ |___ |__| |  \\ |___ ___ |_|_| |___ |__] ___ |  | |__/ \n",
      "                                                                    \n",
      "\n",
      "('Rome', 'Q220', 'LOC', ['capital and largest city of Italy'], 0.6630820438808648)\n",
      "____ _  _     ____ ____ ____ ____     _ _ _ ____ ___      _    ____ \n",
      "|___ |\\ |     |    |  | |__/ |___     | | | |___ |__]     |    | __ \n",
      "|___ | \\| ___ |___ |__| |  \\ |___ ___ |_|_| |___ |__] ___ |___ |__] \n",
      "                                                                    \n",
      "\n",
      "('Rome', 'Q220', 'LOC', ['capital and largest city of Italy'], 0.6630820438808648)\n"
     ]
    }
   ],
   "source": [
    "read(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f09f93f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllr}\n",
      "\\toprule\n",
      "modèle & span & label  & description & ID & score \\\\\n",
      "\\midrule\n",
      "fr_core_news_sm & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "fr_core_news_md & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "fr_core_news_lg & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "en_core_web_sm & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "en_core_web_md & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "en_core_web_lg & Rome & LOC & ['capital and largest city of Italy'] & Q220 & 0.663082 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latexiser_mod(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af412fa",
   "metadata": {},
   "source": [
    "# Audoux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d3c63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref =\"Quand il ne fit plus froid dans la classe, sœur Marie-Aimée me plaça, sur un banc entre Ismérie et Marie Renaud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee582310",
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken = \"Quand il ne fit plus froid dans la classe, seur Marie-Aimee me plagca sur un banc entre Ismerie et Marie Renaud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3115cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Marie-Aimée', Entity not linked to a knowledge base, 'PERSON'], ['Ismérie', Entity not linked to a knowledge base, 'PERSON'], ['Marie Renaud', Entity(wikidata_entity_id=Q19880806, wikipedia_entity_title=Marie Renaud), 'PERSON']]\n"
     ]
    }
   ],
   "source": [
    "afficher(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9e254e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Marie-Aimee', Entity not linked to a knowledge base, 'PERSON'], ['Ismerie', Entity not linked to a knowledge base, 'PERSON'], ['Marie Renaud', Entity(wikidata_entity_id=Q19880806, wikipedia_entity_title=Marie Renaud), 'PERSON']]\n"
     ]
    }
   ],
   "source": [
    "afficher(kraken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a49277e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "read(\"cjdvnqsdvbùv\",\"fr_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac548a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce n'est pas la REN de Spacy car les catégories \n",
    "supprimer ce qui a certitude faible comme  \"vell\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
