{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8facbf",
   "metadata": {},
   "source": [
    "16/08/2023         \n",
    "objectif : évaluer l'influence du contexte sur le liage avec Refined    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07fbd9",
   "metadata": {},
   "source": [
    "# Test du système Refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7afe4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refined (txt):\n",
    "    from refined.inference.processor import Refined\n",
    "\n",
    "\n",
    "    refined = Refined.from_pretrained(model_name='wikipedia_model_with_numbers',\n",
    "                                  entity_set=\"wikipedia\")\n",
    "\n",
    "    spans = refined.process_text(txt)\n",
    "\n",
    "    print(spans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45eebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fishing(texte):\n",
    "    tab_entite =[]\n",
    "    nlp = spacy.load(\"fr_core_news_md\")\n",
    "    \n",
    "    nlp.add_pipe(\"entityfishing\")\n",
    "    doc = nlp(texte)\n",
    "        # returns all entities in the whole document\n",
    "        # iterates over sentences and prints linked entities\n",
    "    for ent in doc.ents:\n",
    "        tab_entite.append({\n",
    "            #\"modèle\": mod,\n",
    "            \"span\" : ent.text,\n",
    "            \"label \" : ent.label_,\n",
    "            \"URL\" : ent._.url_wikidata,\n",
    "            #\"ID\" : ent._.kb_qid,\n",
    "            \"score\" : ent._.nerd_score,\n",
    "            })          \n",
    "    df = pd.DataFrame(tab_entite)\n",
    "    table_latex = df.to_latex(index=False)\n",
    "    latex = df.to_latex(index=False)\n",
    "    print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166b1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opentapioca(texte):\n",
    "    tab_entite =[]\n",
    "    nlp = spacy.load(\"fr_core_news_lg\")\n",
    "    \n",
    "    nlp.add_pipe(\"opentapioca\")\n",
    "    doc = nlp(texte)\n",
    "        # returns all entities in the whole document\n",
    "        # iterates over sentences and prints linked entities\n",
    "    for span in doc.ents:\n",
    "        tab_entite.append({\n",
    "            #\"modèle\": model,\n",
    "            \"span\" : span.text,\n",
    "            \"label \" : span.label_,\n",
    "            \"description\" : span._.description,\n",
    "            #\"URL\" : ent._.url_wikidata,\n",
    "            \"ID\" : span.kb_id_ ,\n",
    "            #\"ID\" : ident,\n",
    "            \"score\" : span._.score,\n",
    "            })   \n",
    "    df = pd.DataFrame(tab_entite)\n",
    "    #print(df.info())\n",
    "    table_latex = df.to_latex(index=False)\n",
    "    latex = df.to_latex(index=False)\n",
    "    print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9efbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linker (texte):\n",
    "    nlp = spacy.load(modele)\n",
    "# add pipeline (declared through entry_points in setup.py)\n",
    "    nlp.add_pipe(\"entityLinker\", last=True)\n",
    "    doc = nlp(texte)\n",
    "# returns all entities in the whole document\n",
    "    all_linked_entities = doc._.linkedEntities\n",
    "# iterates over sentences and prints linked entities\n",
    "    for sent in doc.sents:\n",
    "        sent._.linkedEntities.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b7e820",
   "metadata": {},
   "source": [
    "## Pierre "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6518ca",
   "metadata": {},
   "source": [
    "### not linked to knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef55936",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"Je crois même qu'elle avait peur que Pierre ne les compte, ces baisers-là. \")\n",
    "refined(\"Un chacun dit que c'est le départ de Pierre qui lui a donné cette maladie-là.\")\n",
    "refined(\"Pierre, prenant une grande résolution, dit comme ça, tout d'une haleine : \")\n",
    "refined(\"je ne pourrais t'oublier, quand même je m'y appliquerais, car je t'aime, Pierre, je t'aime à plein cœur. \")\n",
    "refined(\"Pierre, sur ce qu'il y a de plus sacré, jure-moi... \") \n",
    "refined(\"M'est avis aussi, Pierre, qu'on a deux cœurs, et je te garderai celui que tu me garderas. \") \n",
    "refined(\" Tu parles comme monsieur le curé, dit Pierre\")\n",
    "refined(\"Puis, se levant de sa chaise, Pierre s'en vint embrasser sa promise. \"\n",
    "refined(\" — Pierre, dit la Rose d'une voix sûre, et tout émotionnée en même temps, demande-moi plutôt si je respecte ma mère, si je regrette défunt mon père ; mais, viens, viens ! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf252778",
   "metadata": {},
   "source": [
    "### Pierre Corneille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a3dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\" Tu soutiens, Pierre, dit la fillette, que les bonnets blancs doivent plus d'amitié et de fidélité aux hommes, que les hommes aux bonnets blancs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b741b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"Il lui dit tout : la trahison de Pierre, les méchancetés des gens du village, et, pour dernière preuve, que M. Céran venait de lui confier en catimini que si on ne parvenait pas à remonter le moral de la Rose, la fillette serait morte avant un mois. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463e474",
   "metadata": {},
   "source": [
    "### Pierre Cochereau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e1f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Pierre', Entity(wikidata_entity_id=Q1187725, wikipedia_entity_title=Pierre Cochereau), 'PERSON'], ['nds garde', Entity not linked to a knowledge base, 'WORK_OF_ART'], ['bonne amie', Entity not linked to a knowledge base, 'WORK_OF_ART']]\n"
     ]
    }
   ],
   "source": [
    "refined (\"— Pierre, prends garde à ta bonne amie\")\n",
    "refined(\"— Oh ! ne te fâche point, ma Rose, dit Pierre \") \n",
    "refined(\"— Pierre, laisse-moi, va-t-en... ! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57458a4b",
   "metadata": {},
   "source": [
    "### erreurs de REN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c6ce673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Jure', Entity(wikidata_entity_id=Q132555, wikipedia_entity_title=De jure), None], ['Pierre, et dépêche-toi', Entity not linked to a knowledge base, 'WORK_OF_ART']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Rose', Entity not linked to a knowledge base, None], ['Pierre loin', Entity not linked to a knowledge base, 'PERSON'], ['elle', Entity(wikidata_entity_id=Q154020, wikipedia_entity_title=Elle (magazine)), None]]\n"
     ]
    }
   ],
   "source": [
    "refined (\"Jure, Pierre, et dépêche-toi. \") \n",
    "refined (\"Ayant répondu, la Rose se mit à courir du côté de sa chambre en repoussant Pierre loin d'elle, à mesure qu'il l'approchait. \")\n",
    "refined(\"Un chacun tombe d'accord quand on avance que cet enfant-là  ne peut être que de Pierre le dragon. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b16600",
   "metadata": {},
   "outputs": [],
   "source": [
    "## entités non liables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"Il faut vous dire que tout le monde est retourné du côté de la maîtresse à Pierre le dragon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269dfe7",
   "metadata": {},
   "source": [
    "## le père Roux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825e2e5c",
   "metadata": {},
   "source": [
    "### not linked to knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f03066",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"— Qu'est-ce donc que tu avais fait de ton briquet ? dit le père Roux. \")\n",
    "refined(\"Tout de même, si nous arrivions à ne pas payer plus cher, pour nous rendre en moisson, que le père Roux, berger, lors de son voyage dans Paris, nous en serions rudement facilités. \")\n",
    "refined(\"si on pouvait arriver à ne pas payer plus cher pour aller à la France que le père Roux, berger, lors de son voyage à Paris, c'est ça qui serait joliment avantageux. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9916301",
   "metadata": {},
   "source": [
    "### trop long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"le père Roux nous a remis au dimanche suivant. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a59777",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined( \"comme nous étions réunis chez Clarisse Manot, le père Roux a commencé par dire :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f953193",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"— L'hiver viendra sans oraisons, dit le père Roux, berger, en faisant trotter ses aiguilles plus vite que ses moutons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"— Voilà une preuve qu'on peut s'aimer tout de même dans le mariage, dit le fieu du père Roux, berger, à l'oreille de la Rose. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\" Tu deviens par trop bravache, petiot Roux, dit Norine ; tâche un peu de te tenir tranquille. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9526e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"— Retiens celle-là, hé ! petiote Perpétue, dit le père Roux, berger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\" Faut lui faire raconter son histoire du renard, dit le père Roux à Jean-Claude, ça nous amusera ;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"— Oui, faudrait entendre, dit le père Roux. Allons, allons, Pierre, en route ! \")\n",
    "refined(\"Le père Roux, berger, soutient que la lune a un grain ou deux\")\n",
    "refined(\"— Comme environ les peupliers de notre cimetière, a dit le père Roux ;\")\n",
    "refined(\"Faut pas entamer ce chapitre-là avec le père Roux, on le met dans des fureurs à n'en point finir.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639fbf6",
   "metadata": {},
   "source": [
    "### Jacques Roux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac098f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"— Je m'attends, répondit encore le père Roux, que ça n'est point les imitations d'étoiles que j'ai reluquées au plancher qui auront pu donner une pareille idée \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7d299e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Roux', Entity(wikidata_entity_id=Q615492, wikipedia_entity_title=Jacques Roux), None], ['Gaspard', Entity not linked to a knowledge base, None]]\n"
     ]
    }
   ],
   "source": [
    "refined(\"— Je défends qu'on chante la conscription, dit le père Roux, ça me ferait songer à mon Gaspard pour le même motif. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25226a29",
   "metadata": {},
   "source": [
    "## Roux en Belgique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78823e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ades', Entity not linked to a knowledge base, 'GPE'], ['Saint-Brunelle', Entity not linked to a knowledge base, 'GPE'], ['ère', Entity(wikidata_entity_id=Q207143, wikipedia_entity_title=Zlatibor), 'FAC'], ['Roux', Entity(wikidata_entity_id=Q900429, wikipedia_entity_title=Roux, Belgium), 'GPE'], ['berger', Entity not linked to a knowledge base, 'ORG']]\n"
     ]
    }
   ],
   "source": [
    "refined(\"Sur trente-huit malades, il n'est mort à Saint-Brunelle que le père Roux, berger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ffb759",
   "metadata": {},
   "source": [
    "### les faux-négatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2076d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "refined(\" tu l'as prouvé, père Roux, mon camarade ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f848d574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Clarisse Manot', Entity not linked to a knowledge base, 'PERSON'], ['père Roux', Entity not linked to a knowledge base, None]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3622b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['le père Roux', Entity not linked to a knowledge base, None]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a086224",
   "metadata": {},
   "source": [
    "## Savoyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "### trop long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f3e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b132389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mairesse', Entity(wikidata_entity_id=Q171964, wikipedia_entity_title=Guy Mairesse), None], ['iot Savoyard', Entity not linked to a knowledge base, 'PERSON']]\n"
     ]
    }
   ],
   "source": [
    "refined(\"Madame la mairesse parlait, ce dernier soir, de la marier à temps et heure avec le petiot Savoyard. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462085d4",
   "metadata": {},
   "source": [
    "### not linked to knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a25315a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Savoyard-là', Entity not linked to a knowledge base, None]]\n"
     ]
    }
   ],
   "source": [
    "refined(\"M'est avis qu'il est bon que vous appreniez aussi l'histoire de ce Savoyard-là : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d281e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at /home/antonomaz/.cache/refined/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['semble-t-il', None, 'DATE'], ['parigote', Entity not linked to a knowledge base, None], ['Savoyard', Entity not linked to a knowledge base, None]]\n"
     ]
    }
   ],
   "source": [
    "refined(\"Ne vous semble-t-il pas que la parigote et le Savoyard seraient bien unis ensemble ? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef739ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ad39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bda794",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
